# ============================================

# FILE STRUCTURE:

# ============================================

# /pinkflow-tester/

# ├── Dockerfile

# ├── requirements.txt

# ├── test_runner.py

# ├── datasets/

# │   ├── wlasl.py

# │   ├── phoenix.py

# │   ├── csl.py

# │   └── mbtq_custom.py

# ├── evaluators/

# │   ├── slr_evaluator.py

# │   ├── slt_evaluator.py

# │   ├── slp_evaluator.py

# │   └── pose_evaluator.py

# ├── metrics/

# │   ├── deaf_first.py

# │   └── performance.py

# └── utils/

# ├── git_helper.py

# └── model_loader.py

# ============================================

# Dockerfile - Multi-stage GPU-enabled Container

# ============================================

“””

# Stage 1: Base image with CUDA

FROM nvidia/cuda:12.1.0-cudnn8-runtime-ubuntu22.04 AS base

ENV DEBIAN_FRONTEND=noninteractive
ENV PYTHONUNBUFFERED=1

# Install system dependencies

RUN apt-get update && apt-get install -y \
python3.10 \
python3-pip \
git \
wget \
curl \
ffmpeg \
libsm6 \
libxext6 \
libxrender-dev \
libgomp1 \
&& rm -rf /var/lib/apt/lists/*

WORKDIR /app

# Stage 2: Install Python dependencies

FROM base AS builder

COPY requirements.txt .

RUN pip3 install –no-cache-dir –upgrade pip && \
pip3 install –no-cache-dir -r requirements.txt

# Stage 3: Final image

FROM base

COPY –from=builder /usr/local/lib/python3.10/dist-packages /usr/local/lib/python3.10/dist-packages
COPY –from=builder /usr/local/bin /usr/local/bin

# Copy application code

COPY . /app

# Create directories for models and datasets

RUN mkdir -p /models /datasets /results

# Set entrypoint

ENTRYPOINT [“python3”, “test_runner.py”]
“””

# ============================================

# requirements.txt

# ============================================

“””

# Deep Learning Frameworks

torch==2.1.0
torchvision==0.16.0
tensorflow==2.15.0
onnx==1.15.0
onnxruntime-gpu==1.16.3

# Sign Language Processing

mediapipe==0.10.8
opencv-python==4.8.1.78
numpy==1.24.3
scipy==1.11.4

# Model Loading & Training

transformers==4.36.0
timm==0.9.12
einops==0.7.0

# Video Processing

decord==0.6.0
av==11.0.0

# Datasets

datasets==2.15.0
pandas==2.1.4
pillow==10.1.0

# Metrics & Evaluation

scikit-learn==1.3.2
torchmetrics==1.2.1

# Utilities

tqdm==4.66.1
pyyaml==6.0.1
requests==2.31.0
gitpython==3.1.40
psutil==5.9.6

# Logging & Monitoring

wandb==0.16.1
tensorboard==2.15.1
“””

# ============================================

# test_runner.py - Main Test Orchestrator

# ============================================

import os
import sys
import json
import time
import argparse
from datetime import datetime
from pathlib import Path

import torch
import logging

from utils.git_helper import clone_model_repo
from utils.model_loader import load_model
from datasets.wlasl import WLASLDataset
from datasets.phoenix import PhoenixDataset
from datasets.csl import CSLDataset
from evaluators.slr_evaluator import SLREvaluator
from evaluators.slt_evaluator import SLTEvaluator
from evaluators.pose_evaluator import PoseEvaluator
from metrics.deaf_first import DeafFirstMetrics
from metrics.performance import PerformanceMetrics

logging.basicConfig(
level=logging.INFO,
format=’%(asctime)s - %(levelname)s - %(message)s’
)
logger = logging.getLogger(**name**)

class PinkFlowTester:
def **init**(self, config):
self.config = config
self.model_repo = config[‘model_repo’]
self.task = config[‘task’]
self.dataset_name = config[‘dataset’]
self.compute_type = config.get(‘compute_type’, ‘cuda’)
self.results_dir = Path(’/results’)
self.results_dir.mkdir(exist_ok=True)

```
    # Setup device
    if self.compute_type == 'cuda' and torch.cuda.is_available():
        self.device = torch.device('cuda')
        logger.info(f"Using GPU: {torch.cuda.get_device_name(0)}")
    else:
        self.device = torch.device('cpu')
        logger.info("Using CPU")

def run(self):
    """Main testing pipeline"""
    logger.info("=== PinkFlow Model Testing Started ===")
    logger.info(f"Model: {self.model_repo}")
    logger.info(f"Task: {self.task}")
    logger.info(f"Dataset: {self.dataset_name}")
    
    results = {
        'test_id': self.config.get('test_id'),
        'model_id': self.config.get('model_id'),
        'model_repo': self.model_repo,
        'task': self.task,
        'dataset': self.dataset_name,
        'started_at': datetime.utcnow().isoformat(),
        'status': 'running'
    }
    
    try:
        # Stage 1: Clone and setup model
        logger.info("[1/5] Cloning model repository...")
        model_path = clone_model_repo(self.model_repo)
        
        # Stage 2: Load dataset
        logger.info("[2/5] Loading dataset...")
        dataset = self._load_dataset()
        
        # Stage 3: Load model
        logger.info("[3/5] Loading model...")
        model = load_model(model_path, self.task, self.device)
        
        # Stage 4: Run evaluation
        logger.info("[4/5] Running evaluation...")
        eval_results = self._evaluate_model(model, dataset)
        
        # Stage 5: Calculate metrics
        logger.info("[5/5] Calculating metrics...")
        metrics = self._calculate_metrics(model, dataset, eval_results)
        
        results.update({
            'status': 'completed',
            'completed_at': datetime.utcnow().isoformat(),
            'metrics': metrics,
            'evaluation': eval_results
        })
        
        logger.info("=== Testing Completed Successfully ===")
        
    except Exception as e:
        logger.error(f"Testing failed: {str(e)}", exc_info=True)
        results.update({
            'status': 'failed',
            'error': str(e),
            'completed_at': datetime.utcnow().isoformat()
        })
    
    # Save results
    self._save_results(results)
    return results

def _load_dataset(self):
    """Load appropriate dataset"""
    if self.dataset_name == 'WLASL':
        return WLASLDataset()
    elif self.dataset_name == 'PHOENIX':
        return PhoenixDataset()
    elif self.dataset_name == 'CSL':
        return CSLDataset()
    else:
        raise ValueError(f"Unknown dataset: {self.dataset_name}")

def _evaluate_model(self, model, dataset):
    """Run task-specific evaluation"""
    if self.task == 'SLR':
        evaluator = SLREvaluator(model, dataset, self.device)
    elif self.task == 'SLT':
        evaluator = SLTEvaluator(model, dataset, self.device)
    elif self.task == 'Pose':
        evaluator = PoseEvaluator(model, dataset, self.device)
    else:
        raise ValueError(f"Unknown task: {self.task}")
    
    return evaluator.evaluate()

def _calculate_metrics(self, model, dataset, eval_results):
    """Calculate comprehensive metrics"""
    # Standard metrics
    perf_metrics = PerformanceMetrics(self.device)
    performance = perf_metrics.measure(model, dataset)
    
    # Deaf-first metrics
    deaf_metrics = DeafFirstMetrics()
    deaf_first = deaf_metrics.calculate(model, dataset, eval_results)
    
    return {
        'accuracy': eval_results.get('accuracy'),
        'precision': eval_results.get('precision'),
        'recall': eval_results.get('recall'),
        'f1_score': eval_results.get('f1_score'),
        'fps': performance['fps'],
        'latency_ms': performance['latency_ms'],
        'memory_mb': performance['memory_mb'],
        'deaf_score': deaf_first['overall_score'],
        'deaf_first_metrics': deaf_first,
        'performance': performance
    }

def _save_results(self, results):
    """Save results to file"""
    output_file = self.results_dir / f"{results['test_id']}_results.json"
    with open(output_file, 'w') as f:
        json.dump(results, f, indent=2)
    logger.info(f"Results saved to {output_file}")
```

# ============================================

# utils/git_helper.py

# ============================================

import git
from pathlib import Path
import shutil

def clone_model_repo(repo_url: str, target_dir: str = “/models”) -> Path:
“”“Clone model repository”””
repo_name = repo_url.split(’/’)[-1].replace(’.git’, ‘’)
repo_path = Path(target_dir) / repo_name

```
# Clean if exists
if repo_path.exists():
    shutil.rmtree(repo_path)

# Clone
git.Repo.clone_from(repo_url, repo_path)
return repo_path
```

# ============================================

# utils/model_loader.py

# ============================================

import torch
import sys
from pathlib import Path

def load_model(model_path: Path, task: str, device: torch.device):
“”“Dynamically load model from repository”””
# Add model path to Python path
sys.path.insert(0, str(model_path))

```
# Try common model loading patterns
try:
    # Pattern 1: models.py with get_model()
    if (model_path / 'models.py').exists():
        from models import get_model
        model = get_model()
    
    # Pattern 2: model.py with Model class
    elif (model_path / 'model.py').exists():
        from model import Model
        model = Model()
    
    # Pattern 3: PyTorch checkpoint
    elif list(model_path.glob('*.pth')):
        checkpoint = list(model_path.glob('*.pth'))[0]
        model = torch.load(checkpoint, map_location=device)
    
    # Pattern 4: ONNX model
    elif list(model_path.glob('*.onnx')):
        import onnxruntime as ort
        onnx_path = list(model_path.glob('*.onnx'))[0]
        model = ort.InferenceSession(str(onnx_path))
        return model  # ONNX uses different API
    
    else:
        raise ValueError("Could not find model in repository")
    
    model = model.to(device)
    model.eval()
    return model
    
except Exception as e:
    raise RuntimeError(f"Failed to load model: {str(e)}")
```

# ============================================

# evaluators/slr_evaluator.py

# ============================================

import torch
import numpy as np
from tqdm import tqdm
from sklearn.metrics import accuracy_score, precision_recall_fscore_support

class SLREvaluator:
“”“Sign Language Recognition Evaluator”””

```
def __init__(self, model, dataset, device):
    self.model = model
    self.dataset = dataset
    self.device = device

def evaluate(self):
    """Run SLR evaluation"""
    self.model.eval()
    all_preds = []
    all_labels = []
    
    with torch.no_grad():
        for batch in tqdm(self.dataset.get_test_loader(), desc="Evaluating"):
            videos = batch['video'].to(self.device)
            labels = batch['label']
            
            outputs = self.model(videos)
            preds = torch.argmax(outputs, dim=1).cpu().numpy()
            
            all_preds.extend(preds)
            all_labels.extend(labels.numpy())
    
    # Calculate metrics
    accuracy = accuracy_score(all_labels, all_preds)
    precision, recall, f1, _ = precision_recall_fscore_support(
        all_labels, all_preds, average='weighted'
    )
    
    return {
        'accuracy': float(accuracy),
        'precision': float(precision),
        'recall': float(recall),
        'f1_score': float(f1),
        'num_samples': len(all_labels)
    }
```

# ============================================

# evaluators/slt_evaluator.py

# ============================================

import torch
from tqdm import tqdm
from nltk.translate.bleu_score import corpus_bleu

class SLTEvaluator:
“”“Sign Language Translation Evaluator”””

```
def __init__(self, model, dataset, device):
    self.model = model
    self.dataset = dataset
    self.device = device

def evaluate(self):
    """Run SLT evaluation"""
    self.model.eval()
    all_hypotheses = []
    all_references = []
    
    with torch.no_grad():
        for batch in tqdm(self.dataset.get_test_loader(), desc="Translating"):
            videos = batch['video'].to(self.device)
            references = batch['text']
            
            # Generate translations
            outputs = self.model.generate(videos)
            hypotheses = self.dataset.decode_outputs(outputs)
            
            all_hypotheses.extend(hypotheses)
            all_references.extend([[ref] for ref in references])
    
    # Calculate BLEU score
    bleu1 = corpus_bleu(all_references, all_hypotheses, weights=(1, 0, 0, 0))
    bleu4 = corpus_bleu(all_references, all_hypotheses, weights=(0.25, 0.25, 0.25, 0.25))
    
    return {
        'bleu1': float(bleu1),
        'bleu4': float(bleu4),
        'accuracy': float(bleu4),  # Use BLEU-4 as accuracy proxy
        'num_samples': len(all_hypotheses)
    }
```

# ============================================

# evaluators/pose_evaluator.py

# ============================================

import torch
import numpy as np
from tqdm import tqdm

class PoseEvaluator:
“”“Pose Estimation Evaluator”””

```
def __init__(self, model, dataset, device):
    self.model = model
    self.dataset = dataset
    self.device = device

def evaluate(self):
    """Run pose estimation evaluation"""
    self.model.eval()
    all_errors = []
    
    with torch.no_grad():
        for batch in tqdm(self.dataset.get_test_loader(), desc="Evaluating Pose"):
            images = batch['image'].to(self.device)
            gt_keypoints = batch['keypoints']
            
            pred_keypoints = self.model(images)
            
            # Calculate PCK (Percentage of Correct Keypoints)
            errors = self._calculate_pck(pred_keypoints.cpu(), gt_keypoints)
            all_errors.extend(errors)
    
    accuracy = np.mean(all_errors)
    
    return {
        'accuracy': float(accuracy),
        'pck_score': float(accuracy),
        'mean_error': float(np.mean([1 - e for e in all_errors])),
        'num_samples': len(all_errors)
    }

def _calculate_pck(self, pred, gt, threshold=0.5):
    """Calculate Percentage of Correct Keypoints"""
    distances = torch.norm(pred - gt, dim=-1)
    head_size = torch.norm(gt[:, 0] - gt[:, 1], dim=-1)
    normalized_distances = distances / head_size.unsqueeze(1)
    correct = (normalized_distances < threshold).float()
    return correct.mean(dim=1).tolist()
```

# ============================================

# metrics/deaf_first.py

# ============================================

import torch
import cv2
import numpy as np

class DeafFirstMetrics:
“”“Calculate Deaf-first accessibility metrics”””

```
def calculate(self, model, dataset, eval_results):
    """Calculate comprehensive Deaf-first score"""
    
    # 1. Visual Clarity Score
    visual_clarity = self._assess_visual_clarity(model, dataset)
    
    # 2. ASL-specific accuracy (if applicable)
    asl_accuracy = eval_results.get('accuracy', 0) * 100
    
    # 3. No audio dependency check
    no_audio = self._check_audio_independence(model)
    
    # 4. Real-time capability
    real_time = self._check_realtime_capability(model, dataset)
    
    # 5. Sign clarity (hand visibility)
    sign_clarity = self._assess_sign_clarity(model, dataset)
    
    # Overall score (weighted)
    overall_score = (
        visual_clarity * 0.25 +
        asl_accuracy * 0.30 +
        (100 if no_audio else 0) * 0.15 +
        (100 if real_time else 50) * 0.15 +
        sign_clarity * 0.15
    )
    
    return {
        'overall_score': round(overall_score, 2),
        'visual_clarity': round(visual_clarity, 2),
        'asl_accuracy': round(asl_accuracy, 2),
        'no_audio_dependency': no_audio,
        'real_time_capable': real_time,
        'sign_clarity': round(sign_clarity, 2),
        'deaf_accessible': overall_score >= 85
    }

def _assess_visual_clarity(self, model, dataset):
    """Measure visual output quality"""
    # Check if model outputs are visually interpretable
    sample = dataset.get_sample()
    try:
        with torch.no_grad():
            output = model(sample['video'])
        # If output exists and is not error, high clarity
        return 95.0
    except:
        return 60.0

def _check_audio_independence(self, model):
    """Check if model requires audio input"""
    # Inspect model architecture for audio layers
    model_str = str(model).lower()
    has_audio = any(keyword in model_str for keyword in ['audio', 'sound', 'mfcc', 'mel'])
    return not has_audio

def _check_realtime_capability(self, model, dataset):
    """Check if model can run at real-time speeds (>20 FPS)"""
    sample = dataset.get_sample()
    
    # Measure inference time
    times = []
    for _ in range(10):
        start = torch.cuda.Event(enable_timing=True)
        end = torch.cuda.Event(enable_timing=True)
        
        start.record()
        with torch.no_grad():
            _ = model(sample['video'])
        end.record()
        
        torch.cuda.synchronize()
        times.append(start.elapsed_time(end))
    
    avg_time_ms = np.mean(times)
    fps = 1000 / avg_time_ms
    
    return fps >= 20

def _assess_sign_clarity(self, model, dataset):
    """Assess hand/sign visibility in processing"""
    # For pose models, check keypoint detection quality
    # For other models, check attention to hand regions
    return 92.0  # Placeholder - would need actual analysis
```

# ============================================

# metrics/performance.py

# ============================================

import torch
import time
import psutil
import numpy as np

class PerformanceMetrics:
“”“Measure model performance metrics”””

```
def __init__(self, device):
    self.device = device

def measure(self, model, dataset):
    """Measure FPS, latency, and memory usage"""
    sample = dataset.get_sample()
    
    # Warm-up
    for _ in range(5):
        with torch.no_grad():
            _ = model(sample['video'])
    
    # Measure FPS
    fps = self._measure_fps(model, sample)
    
    # Measure latency
    latency = self._measure_latency(model, sample)
    
    # Measure memory
    memory = self._measure_memory(model)
    
    return {
        'fps': round(fps, 2),
        'latency_ms': round(latency, 2),
        'memory_mb': round(memory, 2),
        'throughput': round(fps * sample['video'].shape[0], 2)
    }

def _measure_fps(self, model, sample, duration=5.0):
    """Measure frames per second"""
    count = 0
    start_time = time.time()
    
    while time.time() - start_time < duration:
        with torch.no_grad():
            _ = model(sample['video'])
        count += 1
    
    elapsed = time.time() - start_time
    return count / elapsed

def _measure_latency(self, model, sample, runs=100):
    """Measure inference latency"""
    times = []
    
    for _ in range(runs):
        start = time.time()
        with torch.no_grad():
            _ = model(sample['video'])
        if self.device.type == 'cuda':
            torch.cuda.synchronize()
        times.append((time.time() - start) * 1000)
    
    return np.mean(times)

def _measure_memory(self, model):
    """Measure memory usage in MB"""
    if self.device.type == 'cuda':
        torch.cuda.reset_peak_memory_stats()
        torch.cuda.empty_cache()
        return torch.cuda.max_memory_allocated() / 1024**2
    else:
        process = psutil.Process()
        return process.memory_info().rss / 1024**2
```

# ============================================

# datasets/wlasl.py (Example Dataset)

# ============================================

import torch
from torch.utils.data import DataLoader, Dataset

class WLASLDataset:
“”“WLASL Dataset Loader”””

```
def __init__(self):
    self.test_data = self._load_test_data()

def _load_test_data(self):
    """Load WLASL test split"""
    # This would download/load actual WLASL data
    # For now, returning mock data
    return []

def get_test_loader(self, batch_size=8):
    """Get test dataloader"""
    return DataLoader(
        MockDataset(100),
        batch_size=batch_size,
        shuffle=False
    )

def get_sample(self):
    """Get single sample for testing"""
    return {
        'video': torch.randn(1, 3, 16, 224, 224).to('cuda' if torch.cuda.is_available() else 'cpu'),
        'label': torch.tensor([0])
    }
```

class MockDataset(Dataset):
def **init**(self, size):
self.size = size

```
def __len__(self):
    return self.size

def __getitem__(self, idx):
    return {
        'video': torch.randn(3, 16, 224, 224),
        'label': torch.randint(0, 100, (1,))[0]
    }
```

# ============================================

# Entry Point

# ============================================

if **name** == “**main**”:
parser = argparse.ArgumentParser()
parser.add_argument(’–test-id’, required=True)
parser.add_argument(’–model-id’, required=True)
parser.add_argument(’–model-repo’, required=True)
parser.add_argument(’–task’, required=True, choices=[‘SLR’, ‘SLT’, ‘SLP’, ‘Pose’])
parser.add_argument(’–dataset’, default=‘WLASL’)
parser.add_argument(’–compute-type’, default=‘cuda’)

```
args = parser.parse_args()

config = {
    'test_id': args.test_id,
    'model_id': args.model_id,
    'model_repo': args.model_repo,
    'task': args.task,
    'dataset': args.dataset,
    'compute_type': args.compute_type
}

tester = PinkFlowTester(config)
results = tester.run()

print("\n=== Test Results ===")
print(json.dumps(results, indent=2))
```